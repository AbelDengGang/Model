{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, inputs=[], name=None, is_trainable=True):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        self.name = name\n",
    "        self.is_trainable = is_trainable\n",
    "        \n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "        \n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder(Node):\n",
    "    def __init__(self, name, is_trainable=True):\n",
    "        Node.__init__(self, name=name, is_trainable=is_trainable)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        if value is not None: self.value = value\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradients = {}\n",
    "        for n in self.outputs:\n",
    "            self.gradients[self] = n.gradients[self] * 1\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, x=None, weigth=None, bias=None, name=None, is_trainable=False):\n",
    "        Node.__init__(self, [x, weigth, bias], name=name, is_trainable=is_trainable)\n",
    "        \n",
    "    def forward(self):\n",
    "        k, x, b = self.inputs[1], self.inputs[0], self.inputs[2]\n",
    "        self.value = k.value * x.value + b.value\n",
    "        \n",
    "    def backward(self):\n",
    "        k, x, b = self.inputs[1], self.inputs[0], self.inputs[2]\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            self.gradients[k] = grad_cost * x.value\n",
    "            \n",
    "            self.gradients[x] = grad_cost * k.value\n",
    "            \n",
    "            self.gradients[b] = grad_cost * 1\n",
    "    \n",
    "        \n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, x, name=None, is_trainable=False):\n",
    "        Node.__init__(self, [x], name=name, is_trainable=is_trainable)\n",
    "        self.x = self.inputs[0]\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x.value)\n",
    "        \n",
    "    def partial(self):\n",
    "        return self._sigmoid(self.x.value) * (1 - self._sigmoid(self.x.value))\n",
    "    \n",
    "    def backward(self):\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.x] = grad_cost * self.partial() \n",
    "    #    print(self.gradients)\n",
    "    \n",
    "    \n",
    "class Relu(Node):\n",
    "    def __init__(self, x, name=None, is_trainable=False):\n",
    "        Node.__init__(self, [x], name=name, is_trainable=is_trainable)\n",
    "        self.x = x\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = self.x.value * (self.x.value > 0)\n",
    "        \n",
    "    def backward(self):\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.x] = grad_cost * (self.x.value > 0) \n",
    "        \n",
    "\n",
    "class L2_LOSS(Node):\n",
    "    def __init__(self, y, y_hat, name=None, is_trainable=False):\n",
    "        Node.__init__(self, [y, y_hat], name=name, is_trainable=is_trainable)\n",
    "        self.y = y\n",
    "        self.y_hat = y_hat\n",
    "        \n",
    "    def forward(self):        \n",
    "        y_v = np.array(self.y.value)\n",
    "        yhat_v = np.array(self.y_hat.value)\n",
    "        self.value = np.mean((y_v - yhat_v) ** 2)\n",
    "        \n",
    "    def backward(self):\n",
    "        # 1/n sum (y- yhat)**2\n",
    "        y_v = np.array(self.y.value)\n",
    "        yhat_v = np.array(self.y_hat.value)\n",
    "        self.gradients[self.y] = 2 * np.mean((y_v - yhat_v))\n",
    "        self.gradients[self.y_hat] = -2 * np.mean((y_v - yhat_v))\n",
    "     #   print(self.gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toplogic(graph):\n",
    "    sorted_node = []\n",
    "    \n",
    "    while len(graph) > 0: \n",
    "\n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        for n in graph:\n",
    "            all_inputs += graph[n]\n",
    "            all_outputs.append(n)\n",
    "        \n",
    "        all_inputs = set(all_inputs)\n",
    "        all_outputs = set(all_outputs)\n",
    "    \n",
    "        need_remove = all_outputs - all_inputs  # which in all_inputs but not in all_outputs\n",
    "    \n",
    "        if len(need_remove) > 0: \n",
    "            node = random.choice(list(need_remove))\n",
    "            \n",
    "            visited_next = [node]\n",
    "            if len(graph) == 1:  visited_next += graph[node]\n",
    "                \n",
    "            graph.pop(node)\n",
    "            sorted_node += visited_next\n",
    "\n",
    "            for _, links in graph.items():\n",
    "                if node in links: links.remove(node)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return sorted_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort_feed_dict(feed_dict):\n",
    "    graph = convert_feed_dict_to_graph(feed_dict)\n",
    "    \n",
    "    return toplogic(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feed_dict_to_graph(feed_dict):\n",
    "    computing_graph = defaultdict(list)\n",
    "    \n",
    "    nodes = [n for n in feed_dict]\n",
    "    \n",
    "    while nodes:\n",
    "        n = nodes.pop(0) \n",
    "        \n",
    "        if isinstance(n, Placeholder):\n",
    "            n.value = feed_dict[n]\n",
    "        \n",
    "        if n in computing_graph: continue\n",
    "\n",
    "        for m in n.outputs:\n",
    "            computing_graph[n].append(m)\n",
    "            nodes.append(m)\n",
    "    \n",
    "    return computing_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(graph_order, monitor=False):\n",
    "    for node in graph_order:\n",
    "        if monitor: print('forward compuiting -- {}'.format(node))\n",
    "        node.forward()\n",
    "        \n",
    "def backward(graph_order, monitor=False):\n",
    "    for node in graph_order[::-1]:\n",
    "        if monitor: print('backward computing -- {}'.format(node))\n",
    "        node.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch(graph_order, monitor=False):\n",
    "    forward(graph_order, monitor)\n",
    "    backward(graph_order, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(graph, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in graph:\n",
    "        if t.is_trainable:\n",
    "            t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gang/anaconda3/envs/YOLO/lib/python3.6/site-packages/ipykernel_launcher.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e332171903424cc39d7fe5b65e730db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_boston()\n",
    "X_, y_ = data['data'], data['target']\n",
    "X_rm = X_[:, 5]\n",
    "\n",
    "w1_, b1_ = np.random.normal(), np.random.normal()\n",
    "w2_, b2_ = np.random.normal(), np.random.normal()\n",
    "w3_, b3_ = np.random.normal(), np.random.normal()\n",
    "\n",
    "\n",
    "X, y = Placeholder(name='X', is_trainable=False), Placeholder(name='y', is_trainable=False)\n",
    "w1, b1 = Placeholder(name='w1'), Placeholder(name='b1')\n",
    "w2, b2 = Placeholder(name='w2'), Placeholder(name='b2')\n",
    "\n",
    "# build model\n",
    "output1 = Linear(X, w1, b1, name='linear-01')\n",
    "output2 = Sigmoid(output1, name='activation')\n",
    "#output2 = Relu(output1, name='activation')\n",
    "y_hat = Linear(output2, w2, b2, name='y_hat')\n",
    "cost = L2_LOSS(y, y_hat, name='cost')\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_rm,\n",
    "    y: y_,\n",
    "    w1: w1_,\n",
    "    w2: w2_,\n",
    "    b1: b1_,\n",
    "    b2: b2_,\n",
    "}\n",
    "\n",
    "graph_sort = topological_sort_feed_dict(feed_dict)\n",
    "\n",
    "epoch = 1000\n",
    "\n",
    "batch_num = len(X_rm)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "losses = []\n",
    "\n",
    "for e in tqdm_notebook(range(epoch)):\n",
    "    loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        index = np.random.choice(range(len(X_rm)))\n",
    "        X.value = X_rm[index]\n",
    "        y.value = y_[index]\n",
    "    \n",
    "        run_one_epoch(graph_sort, monitor=False)\n",
    "    \n",
    "        optimize(graph_sort, learning_rate)\n",
    "        \n",
    "        loss += cost.value\n",
    "\n",
    "    losses.append(loss / batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb26d32a438>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate(x, graph):\n",
    "    X.value = x\n",
    "    forward(graph)\n",
    "    return y_hat.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate(7, graph_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_rm, y_)\n",
    "plt.scatter(X_rm, [predicate(x, graph_sort) for x in X_rm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多维向量版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Placeholder(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def toplogic(graph):\n",
    "    sorted_node = []\n",
    "    \n",
    "    while len(graph) > 0: \n",
    "\n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        for n in graph:\n",
    "            all_inputs += graph[n]\n",
    "            all_outputs.append(n)\n",
    "        \n",
    "        all_inputs = set(all_inputs)\n",
    "        all_outputs = set(all_outputs)\n",
    "    \n",
    "        need_remove = all_outputs - all_inputs  # which in all_inputs but not in all_outputs\n",
    "    \n",
    "        if len(need_remove) > 0: \n",
    "            node = random.choice(list(need_remove))\n",
    "\n",
    "            need_to_visited = [node]\n",
    "\n",
    "            if len(graph) == 1: need_to_visited += graph[node]\n",
    "                \n",
    "            graph.pop(node)\n",
    "            sorted_node += need_to_visited\n",
    "        \n",
    "            for _, links in graph.items():\n",
    "                if node in links: links.remove(node)\n",
    "        else: # have cycle\n",
    "            break\n",
    "        \n",
    "    return sorted_node\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def convert_feed_dict_to_graph(feed_dict):\n",
    "    computing_graph = defaultdict(list)\n",
    "    \n",
    "    nodes = [n for n in feed_dict]\n",
    "    \n",
    "    while nodes:\n",
    "        n = nodes.pop(0) \n",
    "        \n",
    "        if isinstance(n, Placeholder):\n",
    "            n.value = feed_dict[n]\n",
    "        \n",
    "        if n in computing_graph: continue\n",
    "\n",
    "        for m in n.outputs:\n",
    "            computing_graph[n].append(m)\n",
    "            nodes.append(m)\n",
    "    \n",
    "    return computing_graph\n",
    "\n",
    "def topological_sort_feed_dict(feed_dict):\n",
    "    graph = convert_feed_dict_to_graph(feed_dict)\n",
    "    \n",
    "    return toplogic(graph)\n",
    "\n",
    "\n",
    "def optimize(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Placeholder(), Placeholder()\n",
    "W1, b1 = Placeholder(), Placeholder()\n",
    "W2, b2 = Placeholder(), Placeholder()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort_feed_dict(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "#         forward_and_backward(_, graph) # set output node not important.\n",
    "        forward_and_backward(graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        optimize(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss/steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Dimensions Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data['data'])\n",
    "dataframe.columns = data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataframe[['RM', 'LSTAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = training_data\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Placeholder(), Placeholder()\n",
    "W1, b1 = Placeholder(), Placeholder()\n",
    "W2, b2 = Placeholder(), Placeholder()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 200\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 1\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort_feed_dict(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "    \n",
    "        forward_and_backward(graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        optimize(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss/steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.value = np.array([[6, 8]])\n",
    "forward_and_backward(graph)\n",
    "graph[-2].value[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = fig.gca(projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Make data.\n",
    "X = X_.values[:, 0]\n",
    "Y = X_.values[:, 1]\n",
    "Z = y_\n",
    "\n",
    "# Plot the surface.\n",
    "rm_and_lstp_price = ax.scatter(X, Y, Z)\n",
    "ax.set_xlabel('RM')\n",
    "ax.set_ylabel('% of lower state')\n",
    "ax.set_zlabel('Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_result = []\n",
    "X = Placeholder()\n",
    "for rm, ls in training_data.values:\n",
    "    X.value = np.array([[rm, ls]])\n",
    "    forward_and_backward(graph)\n",
    "    predicate_result.append(graph[-2].value[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_result = np.array(predicate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Make data.\n",
    "X = X_.values[:, 0]\n",
    "Y = X_.values[:, 1]\n",
    "Z = predicate_result\n",
    "\n",
    "# Plot the surface.\n",
    "rm_and_lstp_price = ax.plot_trisurf(X, Y, Z, color='yellow')\n",
    "\n",
    "ax.set_xlabel('RM')\n",
    "ax.set_ylabel('% of lower state')\n",
    "ax.set_zlabel('Predicated-Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
